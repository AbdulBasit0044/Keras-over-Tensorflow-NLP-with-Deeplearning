{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b this was soul provoking i am an iranian and living in <OOV> 21st century i didn't know that such big <OOV> have been living in such conditions at the time of my grandfather br br you see that today or even in <OOV> on one side of the world a lady or a baby could have everything served for him or her clean and on demand but here 80 years ago people <OOV> their life to go to somewhere with more grass it's really interesting that these <OOV> bear those difficulties to find <OOV> for their sheep but they lose many the sheep on their way br br i praise the americans who accompanied this tribe they were as\n",
      "b\"This was soul-provoking! I am an Iranian, and living in th 21st century, I didn't know that such big tribes have been living in such conditions at the time of my grandfather!<br /><br />You see that today, or even in 1925, on one side of the world a lady or a baby could have everything served for him or her clean and on-demand, but here 80 years ago, people ventured their life to go to somewhere with more grass. It's really interesting that these Persians bear those difficulties to find pasture for their sheep, but they lose many the sheep on their way.<br /><br />I praise the Americans who accompanied this tribe, they were as tough as Bakhtiari people.\"\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - ETA: 6s - loss: 1.8403e-06 - acc: 1.000 - ETA: 4s - loss: 1.2739e-06 - acc: 1.000 - ETA: 4s - loss: 1.2469e-06 - acc: 1.000 - ETA: 3s - loss: 1.2761e-06 - acc: 1.000 - ETA: 3s - loss: 1.2231e-06 - acc: 1.000 - ETA: 3s - loss: 1.2297e-06 - acc: 1.000 - ETA: 3s - loss: 1.2149e-06 - acc: 1.000 - ETA: 3s - loss: 1.2156e-06 - acc: 1.000 - ETA: 3s - loss: 1.2162e-06 - acc: 1.000 - ETA: 3s - loss: 1.2275e-06 - acc: 1.000 - ETA: 3s - loss: 1.2184e-06 - acc: 1.000 - ETA: 3s - loss: 1.2159e-06 - acc: 1.000 - ETA: 3s - loss: 1.2007e-06 - acc: 1.000 - ETA: 3s - loss: 1.1891e-06 - acc: 1.000 - ETA: 3s - loss: 1.1907e-06 - acc: 1.000 - ETA: 3s - loss: 1.1947e-06 - acc: 1.000 - ETA: 3s - loss: 1.1988e-06 - acc: 1.000 - ETA: 3s - loss: 1.1935e-06 - acc: 1.000 - ETA: 3s - loss: 1.1924e-06 - acc: 1.000 - ETA: 3s - loss: 1.1906e-06 - acc: 1.000 - ETA: 3s - loss: 1.1846e-06 - acc: 1.000 - ETA: 3s - loss: 1.1809e-06 - acc: 1.000 - ETA: 3s - loss: 1.1836e-06 - acc: 1.000 - ETA: 3s - loss: 1.1798e-06 - acc: 1.000 - ETA: 3s - loss: 1.1755e-06 - acc: 1.000 - ETA: 3s - loss: 1.1652e-06 - acc: 1.000 - ETA: 3s - loss: 1.1654e-06 - acc: 1.000 - ETA: 3s - loss: 1.1688e-06 - acc: 1.000 - ETA: 3s - loss: 1.1707e-06 - acc: 1.000 - ETA: 3s - loss: 1.1630e-06 - acc: 1.000 - ETA: 3s - loss: 1.1615e-06 - acc: 1.000 - ETA: 3s - loss: 1.1564e-06 - acc: 1.000 - ETA: 3s - loss: 1.1469e-06 - acc: 1.000 - ETA: 3s - loss: 1.1501e-06 - acc: 1.000 - ETA: 3s - loss: 1.1519e-06 - acc: 1.000 - ETA: 2s - loss: 1.1525e-06 - acc: 1.000 - ETA: 2s - loss: 1.1561e-06 - acc: 1.000 - ETA: 2s - loss: 1.1621e-06 - acc: 1.000 - ETA: 2s - loss: 1.1607e-06 - acc: 1.000 - ETA: 2s - loss: 1.1574e-06 - acc: 1.000 - ETA: 2s - loss: 1.1617e-06 - acc: 1.000 - ETA: 2s - loss: 1.1621e-06 - acc: 1.000 - ETA: 2s - loss: 1.1579e-06 - acc: 1.000 - ETA: 2s - loss: 1.1548e-06 - acc: 1.000 - ETA: 2s - loss: 1.1515e-06 - acc: 1.000 - ETA: 2s - loss: 1.1521e-06 - acc: 1.000 - ETA: 2s - loss: 1.1537e-06 - acc: 1.000 - ETA: 2s - loss: 1.1466e-06 - acc: 1.000 - ETA: 1s - loss: 1.1512e-06 - acc: 1.000 - ETA: 1s - loss: 1.1493e-06 - acc: 1.000 - ETA: 1s - loss: 1.1482e-06 - acc: 1.000 - ETA: 1s - loss: 1.1445e-06 - acc: 1.000 - ETA: 1s - loss: 1.1403e-06 - acc: 1.000 - ETA: 1s - loss: 1.1370e-06 - acc: 1.000 - ETA: 1s - loss: 1.1389e-06 - acc: 1.000 - ETA: 1s - loss: 1.1341e-06 - acc: 1.000 - ETA: 1s - loss: 1.1354e-06 - acc: 1.000 - ETA: 1s - loss: 1.1319e-06 - acc: 1.000 - ETA: 1s - loss: 1.1315e-06 - acc: 1.000 - ETA: 1s - loss: 1.1316e-06 - acc: 1.000 - ETA: 1s - loss: 1.1320e-06 - acc: 1.000 - ETA: 1s - loss: 1.1295e-06 - acc: 1.000 - ETA: 1s - loss: 1.1276e-06 - acc: 1.000 - ETA: 1s - loss: 1.1292e-06 - acc: 1.000 - ETA: 1s - loss: 1.1298e-06 - acc: 1.000 - ETA: 0s - loss: 1.1282e-06 - acc: 1.000 - ETA: 0s - loss: 1.1257e-06 - acc: 1.000 - ETA: 0s - loss: 1.1229e-06 - acc: 1.000 - ETA: 0s - loss: 1.1194e-06 - acc: 1.000 - ETA: 0s - loss: 1.1185e-06 - acc: 1.000 - ETA: 0s - loss: 1.1168e-06 - acc: 1.000 - ETA: 0s - loss: 1.1114e-06 - acc: 1.000 - ETA: 0s - loss: 1.1089e-06 - acc: 1.000 - ETA: 0s - loss: 1.1076e-06 - acc: 1.000 - ETA: 0s - loss: 1.1050e-06 - acc: 1.000 - ETA: 0s - loss: 1.1028e-06 - acc: 1.000 - ETA: 0s - loss: 1.1014e-06 - acc: 1.000 - ETA: 0s - loss: 1.0979e-06 - acc: 1.000 - ETA: 0s - loss: 1.0967e-06 - acc: 1.000 - ETA: 0s - loss: 1.0959e-06 - acc: 1.000 - ETA: 0s - loss: 1.0945e-06 - acc: 1.000 - ETA: 0s - loss: 1.0934e-06 - acc: 1.000 - ETA: 0s - loss: 1.0904e-06 - acc: 1.000 - ETA: 0s - loss: 1.0877e-06 - acc: 1.000 - 6s 223us/sample - loss: 1.0872e-06 - acc: 1.0000 - val_loss: 1.1426 - val_acc: 0.8276\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - ETA: 5s - loss: 6.7614e-07 - acc: 1.000 - ETA: 4s - loss: 7.4636e-07 - acc: 1.000 - ETA: 4s - loss: 7.5427e-07 - acc: 1.000 - ETA: 4s - loss: 7.7805e-07 - acc: 1.000 - ETA: 4s - loss: 7.5702e-07 - acc: 1.000 - ETA: 4s - loss: 7.5775e-07 - acc: 1.000 - ETA: 3s - loss: 7.6643e-07 - acc: 1.000 - ETA: 3s - loss: 7.5741e-07 - acc: 1.000 - ETA: 3s - loss: 7.4545e-07 - acc: 1.000 - ETA: 3s - loss: 7.4830e-07 - acc: 1.000 - ETA: 3s - loss: 7.6329e-07 - acc: 1.000 - ETA: 3s - loss: 7.5949e-07 - acc: 1.000 - ETA: 3s - loss: 7.5268e-07 - acc: 1.000 - ETA: 3s - loss: 7.5138e-07 - acc: 1.000 - ETA: 3s - loss: 7.6113e-07 - acc: 1.000 - ETA: 3s - loss: 7.5868e-07 - acc: 1.000 - ETA: 3s - loss: 7.5438e-07 - acc: 1.000 - ETA: 3s - loss: 7.5421e-07 - acc: 1.000 - ETA: 3s - loss: 7.4844e-07 - acc: 1.000 - ETA: 3s - loss: 7.5205e-07 - acc: 1.000 - ETA: 3s - loss: 7.5465e-07 - acc: 1.000 - ETA: 2s - loss: 7.5235e-07 - acc: 1.000 - ETA: 2s - loss: 7.5042e-07 - acc: 1.000 - ETA: 2s - loss: 7.4427e-07 - acc: 1.000 - ETA: 2s - loss: 7.4512e-07 - acc: 1.000 - ETA: 2s - loss: 7.4117e-07 - acc: 1.000 - ETA: 2s - loss: 7.4261e-07 - acc: 1.000 - ETA: 2s - loss: 7.4451e-07 - acc: 1.000 - ETA: 2s - loss: 7.3969e-07 - acc: 1.000 - ETA: 2s - loss: 7.3581e-07 - acc: 1.000 - ETA: 2s - loss: 7.3137e-07 - acc: 1.000 - ETA: 2s - loss: 7.3243e-07 - acc: 1.000 - ETA: 2s - loss: 7.3190e-07 - acc: 1.000 - ETA: 2s - loss: 7.2920e-07 - acc: 1.000 - ETA: 2s - loss: 7.2997e-07 - acc: 1.000 - ETA: 2s - loss: 7.2844e-07 - acc: 1.000 - ETA: 2s - loss: 7.2497e-07 - acc: 1.000 - ETA: 2s - loss: 7.2186e-07 - acc: 1.000 - ETA: 1s - loss: 7.2715e-07 - acc: 1.000 - ETA: 1s - loss: 7.2646e-07 - acc: 1.000 - ETA: 1s - loss: 7.2397e-07 - acc: 1.000 - ETA: 1s - loss: 7.2214e-07 - acc: 1.000 - ETA: 1s - loss: 7.2200e-07 - acc: 1.000 - ETA: 1s - loss: 7.2077e-07 - acc: 1.000 - ETA: 1s - loss: 7.2092e-07 - acc: 1.000 - ETA: 1s - loss: 7.1887e-07 - acc: 1.000 - ETA: 1s - loss: 7.1852e-07 - acc: 1.000 - ETA: 1s - loss: 7.1939e-07 - acc: 1.000 - ETA: 1s - loss: 7.1847e-07 - acc: 1.000 - ETA: 1s - loss: 7.1734e-07 - acc: 1.000 - ETA: 1s - loss: 7.1380e-07 - acc: 1.000 - ETA: 1s - loss: 7.1244e-07 - acc: 1.000 - ETA: 1s - loss: 7.1134e-07 - acc: 1.000 - ETA: 1s - loss: 7.0985e-07 - acc: 1.000 - ETA: 1s - loss: 7.0888e-07 - acc: 1.000 - ETA: 1s - loss: 7.0631e-07 - acc: 1.000 - ETA: 1s - loss: 7.0679e-07 - acc: 1.000 - ETA: 1s - loss: 7.0407e-07 - acc: 1.000 - ETA: 1s - loss: 7.0212e-07 - acc: 1.000 - ETA: 0s - loss: 7.0140e-07 - acc: 1.000 - ETA: 0s - loss: 7.0117e-07 - acc: 1.000 - ETA: 0s - loss: 7.0208e-07 - acc: 1.000 - ETA: 0s - loss: 7.0127e-07 - acc: 1.000 - ETA: 0s - loss: 6.9885e-07 - acc: 1.000 - ETA: 0s - loss: 7.0128e-07 - acc: 1.000 - ETA: 0s - loss: 7.0146e-07 - acc: 1.000 - ETA: 0s - loss: 7.0056e-07 - acc: 1.000 - ETA: 0s - loss: 7.0209e-07 - acc: 1.000 - ETA: 0s - loss: 7.0243e-07 - acc: 1.000 - ETA: 0s - loss: 7.0063e-07 - acc: 1.000 - ETA: 0s - loss: 6.9878e-07 - acc: 1.000 - ETA: 0s - loss: 6.9637e-07 - acc: 1.000 - ETA: 0s - loss: 6.9286e-07 - acc: 1.000 - ETA: 0s - loss: 6.9071e-07 - acc: 1.000 - ETA: 0s - loss: 6.9064e-07 - acc: 1.000 - ETA: 0s - loss: 6.9203e-07 - acc: 1.000 - ETA: 0s - loss: 6.9269e-07 - acc: 1.000 - ETA: 0s - loss: 6.9193e-07 - acc: 1.000 - 5s 210us/sample - loss: 6.9201e-07 - acc: 1.0000 - val_loss: 1.1675 - val_acc: 0.8276\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - ETA: 7s - loss: 4.9546e-07 - acc: 1.000 - ETA: 5s - loss: 4.4414e-07 - acc: 1.000 - ETA: 4s - loss: 4.5654e-07 - acc: 1.000 - ETA: 4s - loss: 4.3140e-07 - acc: 1.000 - ETA: 4s - loss: 4.8434e-07 - acc: 1.000 - ETA: 3s - loss: 4.9622e-07 - acc: 1.000 - ETA: 3s - loss: 4.8280e-07 - acc: 1.000 - ETA: 3s - loss: 4.7559e-07 - acc: 1.000 - ETA: 3s - loss: 4.6207e-07 - acc: 1.000 - ETA: 3s - loss: 4.6026e-07 - acc: 1.000 - ETA: 3s - loss: 4.6063e-07 - acc: 1.000 - ETA: 3s - loss: 4.6336e-07 - acc: 1.000 - ETA: 3s - loss: 4.6132e-07 - acc: 1.000 - ETA: 3s - loss: 4.6090e-07 - acc: 1.000 - ETA: 3s - loss: 4.7124e-07 - acc: 1.000 - ETA: 3s - loss: 4.6546e-07 - acc: 1.000 - ETA: 3s - loss: 4.6775e-07 - acc: 1.000 - ETA: 3s - loss: 4.6581e-07 - acc: 1.000 - ETA: 3s - loss: 4.6420e-07 - acc: 1.000 - ETA: 3s - loss: 4.6406e-07 - acc: 1.000 - ETA: 3s - loss: 4.6420e-07 - acc: 1.000 - ETA: 3s - loss: 4.6664e-07 - acc: 1.000 - ETA: 3s - loss: 4.6545e-07 - acc: 1.000 - ETA: 2s - loss: 4.6414e-07 - acc: 1.000 - ETA: 2s - loss: 4.6298e-07 - acc: 1.000 - ETA: 2s - loss: 4.6211e-07 - acc: 1.000 - ETA: 2s - loss: 4.6194e-07 - acc: 1.000 - ETA: 2s - loss: 4.6298e-07 - acc: 1.000 - ETA: 2s - loss: 4.5862e-07 - acc: 1.000 - ETA: 2s - loss: 4.5776e-07 - acc: 1.000 - ETA: 2s - loss: 4.5635e-07 - acc: 1.000 - ETA: 2s - loss: 4.5900e-07 - acc: 1.000 - ETA: 2s - loss: 4.5899e-07 - acc: 1.000 - ETA: 2s - loss: 4.6006e-07 - acc: 1.000 - ETA: 2s - loss: 4.6131e-07 - acc: 1.000 - ETA: 2s - loss: 4.6116e-07 - acc: 1.000 - ETA: 2s - loss: 4.6096e-07 - acc: 1.000 - ETA: 2s - loss: 4.5799e-07 - acc: 1.000 - ETA: 2s - loss: 4.5935e-07 - acc: 1.000 - ETA: 1s - loss: 4.5910e-07 - acc: 1.000 - ETA: 1s - loss: 4.5734e-07 - acc: 1.000 - ETA: 1s - loss: 4.5894e-07 - acc: 1.000 - ETA: 1s - loss: 4.5810e-07 - acc: 1.000 - ETA: 1s - loss: 4.5865e-07 - acc: 1.000 - ETA: 1s - loss: 4.5751e-07 - acc: 1.000 - ETA: 1s - loss: 4.5629e-07 - acc: 1.000 - ETA: 1s - loss: 4.5541e-07 - acc: 1.000 - ETA: 1s - loss: 4.5217e-07 - acc: 1.000 - ETA: 1s - loss: 4.5125e-07 - acc: 1.000 - ETA: 1s - loss: 4.4999e-07 - acc: 1.000 - ETA: 1s - loss: 4.4900e-07 - acc: 1.000 - ETA: 1s - loss: 4.4861e-07 - acc: 1.000 - ETA: 1s - loss: 4.4692e-07 - acc: 1.000 - ETA: 1s - loss: 4.4572e-07 - acc: 1.000 - ETA: 1s - loss: 4.4586e-07 - acc: 1.000 - ETA: 1s - loss: 4.4666e-07 - acc: 1.000 - ETA: 1s - loss: 4.4696e-07 - acc: 1.000 - ETA: 1s - loss: 4.4621e-07 - acc: 1.000 - ETA: 0s - loss: 4.4556e-07 - acc: 1.000 - ETA: 0s - loss: 4.4578e-07 - acc: 1.000 - ETA: 0s - loss: 4.4508e-07 - acc: 1.000 - ETA: 0s - loss: 4.4489e-07 - acc: 1.000 - ETA: 0s - loss: 4.4381e-07 - acc: 1.000 - ETA: 0s - loss: 4.4403e-07 - acc: 1.000 - ETA: 0s - loss: 4.4267e-07 - acc: 1.000 - ETA: 0s - loss: 4.4219e-07 - acc: 1.000 - ETA: 0s - loss: 4.4084e-07 - acc: 1.000 - ETA: 0s - loss: 4.3958e-07 - acc: 1.000 - ETA: 0s - loss: 4.4133e-07 - acc: 1.000 - ETA: 0s - loss: 4.4165e-07 - acc: 1.000 - ETA: 0s - loss: 4.4046e-07 - acc: 1.000 - ETA: 0s - loss: 4.3912e-07 - acc: 1.000 - ETA: 0s - loss: 4.3799e-07 - acc: 1.000 - ETA: 0s - loss: 4.3830e-07 - acc: 1.000 - ETA: 0s - loss: 4.3796e-07 - acc: 1.000 - ETA: 0s - loss: 4.3850e-07 - acc: 1.000 - 5s 206us/sample - loss: 4.3759e-07 - acc: 1.0000 - val_loss: 1.1905 - val_acc: 0.8276\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - ETA: 5s - loss: 3.2596e-07 - acc: 1.000 - ETA: 4s - loss: 3.0717e-07 - acc: 1.000 - ETA: 4s - loss: 3.4619e-07 - acc: 1.000 - ETA: 4s - loss: 3.1851e-07 - acc: 1.000 - ETA: 4s - loss: 3.1511e-07 - acc: 1.000 - ETA: 3s - loss: 3.0774e-07 - acc: 1.000 - ETA: 3s - loss: 3.0890e-07 - acc: 1.000 - ETA: 3s - loss: 3.0917e-07 - acc: 1.000 - ETA: 3s - loss: 3.1084e-07 - acc: 1.000 - ETA: 3s - loss: 3.0520e-07 - acc: 1.000 - ETA: 3s - loss: 3.0732e-07 - acc: 1.000 - ETA: 3s - loss: 3.0653e-07 - acc: 1.000 - ETA: 3s - loss: 3.0267e-07 - acc: 1.000 - ETA: 3s - loss: 2.9635e-07 - acc: 1.000 - ETA: 3s - loss: 2.9738e-07 - acc: 1.000 - ETA: 2s - loss: 2.9510e-07 - acc: 1.000 - ETA: 2s - loss: 2.9489e-07 - acc: 1.000 - ETA: 2s - loss: 2.9134e-07 - acc: 1.000 - ETA: 2s - loss: 2.8941e-07 - acc: 1.000 - ETA: 2s - loss: 2.8837e-07 - acc: 1.000 - ETA: 2s - loss: 2.9178e-07 - acc: 1.000 - ETA: 2s - loss: 2.9210e-07 - acc: 1.000 - ETA: 2s - loss: 2.9278e-07 - acc: 1.000 - ETA: 2s - loss: 2.9298e-07 - acc: 1.000 - ETA: 2s - loss: 2.9288e-07 - acc: 1.000 - ETA: 2s - loss: 2.9196e-07 - acc: 1.000 - ETA: 2s - loss: 2.9082e-07 - acc: 1.000 - ETA: 2s - loss: 2.9073e-07 - acc: 1.000 - ETA: 2s - loss: 2.8951e-07 - acc: 1.000 - ETA: 2s - loss: 2.8823e-07 - acc: 1.000 - ETA: 2s - loss: 2.8849e-07 - acc: 1.000 - ETA: 2s - loss: 2.8756e-07 - acc: 1.000 - ETA: 2s - loss: 2.8761e-07 - acc: 1.000 - ETA: 2s - loss: 2.8831e-07 - acc: 1.000 - ETA: 2s - loss: 2.8937e-07 - acc: 1.000 - ETA: 2s - loss: 2.8894e-07 - acc: 1.000 - ETA: 2s - loss: 2.8698e-07 - acc: 1.000 - ETA: 1s - loss: 2.8664e-07 - acc: 1.000 - ETA: 1s - loss: 2.8718e-07 - acc: 1.000 - ETA: 1s - loss: 2.8548e-07 - acc: 1.000 - ETA: 1s - loss: 2.8459e-07 - acc: 1.000 - ETA: 1s - loss: 2.8454e-07 - acc: 1.000 - ETA: 1s - loss: 2.8278e-07 - acc: 1.000 - ETA: 1s - loss: 2.8422e-07 - acc: 1.000 - ETA: 1s - loss: 2.8342e-07 - acc: 1.000 - ETA: 1s - loss: 2.8262e-07 - acc: 1.000 - ETA: 1s - loss: 2.8254e-07 - acc: 1.000 - ETA: 1s - loss: 2.8083e-07 - acc: 1.000 - ETA: 1s - loss: 2.8011e-07 - acc: 1.000 - ETA: 1s - loss: 2.7901e-07 - acc: 1.000 - ETA: 1s - loss: 2.7817e-07 - acc: 1.000 - ETA: 1s - loss: 2.7890e-07 - acc: 1.000 - ETA: 1s - loss: 2.7851e-07 - acc: 1.000 - ETA: 1s - loss: 2.7724e-07 - acc: 1.000 - ETA: 1s - loss: 2.7682e-07 - acc: 1.000 - ETA: 0s - loss: 2.7666e-07 - acc: 1.000 - ETA: 0s - loss: 2.7582e-07 - acc: 1.000 - ETA: 0s - loss: 2.7652e-07 - acc: 1.000 - ETA: 0s - loss: 2.7629e-07 - acc: 1.000 - ETA: 0s - loss: 2.7558e-07 - acc: 1.000 - ETA: 0s - loss: 2.7543e-07 - acc: 1.000 - ETA: 0s - loss: 2.7481e-07 - acc: 1.000 - ETA: 0s - loss: 2.7495e-07 - acc: 1.000 - ETA: 0s - loss: 2.7491e-07 - acc: 1.000 - ETA: 0s - loss: 2.7549e-07 - acc: 1.000 - ETA: 0s - loss: 2.7488e-07 - acc: 1.000 - ETA: 0s - loss: 2.7503e-07 - acc: 1.000 - ETA: 0s - loss: 2.7389e-07 - acc: 1.000 - ETA: 0s - loss: 2.7383e-07 - acc: 1.000 - ETA: 0s - loss: 2.7362e-07 - acc: 1.000 - ETA: 0s - loss: 2.7299e-07 - acc: 1.000 - ETA: 0s - loss: 2.7205e-07 - acc: 1.000 - ETA: 0s - loss: 2.7177e-07 - acc: 1.000 - ETA: 0s - loss: 2.7181e-07 - acc: 1.000 - ETA: 0s - loss: 2.7096e-07 - acc: 1.000 - 5s 202us/sample - loss: 2.7069e-07 - acc: 1.0000 - val_loss: 1.2130 - val_acc: 0.8274\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - ETA: 4s - loss: 3.5949e-07 - acc: 1.000 - ETA: 4s - loss: 2.0337e-07 - acc: 1.000 - ETA: 4s - loss: 1.8058e-07 - acc: 1.000 - ETA: 4s - loss: 1.7434e-07 - acc: 1.000 - ETA: 3s - loss: 1.7751e-07 - acc: 1.000 - ETA: 3s - loss: 1.8068e-07 - acc: 1.000 - ETA: 3s - loss: 1.8844e-07 - acc: 1.000 - ETA: 3s - loss: 1.9201e-07 - acc: 1.000 - ETA: 3s - loss: 1.8972e-07 - acc: 1.000 - ETA: 3s - loss: 1.8460e-07 - acc: 1.000 - ETA: 3s - loss: 1.8719e-07 - acc: 1.000 - ETA: 3s - loss: 1.8408e-07 - acc: 1.000 - ETA: 3s - loss: 1.8215e-07 - acc: 1.000 - ETA: 3s - loss: 1.8355e-07 - acc: 1.000 - ETA: 3s - loss: 1.8184e-07 - acc: 1.000 - ETA: 3s - loss: 1.8160e-07 - acc: 1.000 - ETA: 3s - loss: 1.7884e-07 - acc: 1.000 - ETA: 3s - loss: 1.7793e-07 - acc: 1.000 - ETA: 3s - loss: 1.7718e-07 - acc: 1.000 - ETA: 2s - loss: 1.7703e-07 - acc: 1.000 - ETA: 2s - loss: 1.7905e-07 - acc: 1.000 - ETA: 2s - loss: 1.8022e-07 - acc: 1.000 - ETA: 2s - loss: 1.7845e-07 - acc: 1.000 - ETA: 2s - loss: 1.7977e-07 - acc: 1.000 - ETA: 2s - loss: 1.7918e-07 - acc: 1.000 - ETA: 2s - loss: 1.7891e-07 - acc: 1.000 - ETA: 2s - loss: 1.7751e-07 - acc: 1.000 - ETA: 2s - loss: 1.7872e-07 - acc: 1.000 - ETA: 2s - loss: 1.7808e-07 - acc: 1.000 - ETA: 2s - loss: 1.8059e-07 - acc: 1.000 - ETA: 2s - loss: 1.8102e-07 - acc: 1.000 - ETA: 2s - loss: 1.8271e-07 - acc: 1.000 - ETA: 2s - loss: 1.8168e-07 - acc: 1.000 - ETA: 2s - loss: 1.8151e-07 - acc: 1.000 - ETA: 2s - loss: 1.8052e-07 - acc: 1.000 - ETA: 1s - loss: 1.7922e-07 - acc: 1.000 - ETA: 1s - loss: 1.7895e-07 - acc: 1.000 - ETA: 1s - loss: 1.7874e-07 - acc: 1.000 - ETA: 1s - loss: 1.7746e-07 - acc: 1.000 - ETA: 1s - loss: 1.7682e-07 - acc: 1.000 - ETA: 1s - loss: 1.7627e-07 - acc: 1.000 - ETA: 1s - loss: 1.7531e-07 - acc: 1.000 - ETA: 1s - loss: 1.7509e-07 - acc: 1.000 - ETA: 1s - loss: 1.7488e-07 - acc: 1.000 - ETA: 1s - loss: 1.7402e-07 - acc: 1.000 - ETA: 1s - loss: 1.7392e-07 - acc: 1.000 - ETA: 1s - loss: 1.7440e-07 - acc: 1.000 - ETA: 1s - loss: 1.7483e-07 - acc: 1.000 - ETA: 1s - loss: 1.7414e-07 - acc: 1.000 - ETA: 1s - loss: 1.7359e-07 - acc: 1.000 - ETA: 1s - loss: 1.7410e-07 - acc: 1.000 - ETA: 1s - loss: 1.7392e-07 - acc: 1.000 - ETA: 1s - loss: 1.7317e-07 - acc: 1.000 - ETA: 1s - loss: 1.7349e-07 - acc: 1.000 - ETA: 0s - loss: 1.7338e-07 - acc: 1.000 - ETA: 0s - loss: 1.7297e-07 - acc: 1.000 - ETA: 0s - loss: 1.7262e-07 - acc: 1.000 - ETA: 0s - loss: 1.7228e-07 - acc: 1.000 - ETA: 0s - loss: 1.7220e-07 - acc: 1.000 - ETA: 0s - loss: 1.7182e-07 - acc: 1.000 - ETA: 0s - loss: 1.7177e-07 - acc: 1.000 - ETA: 0s - loss: 1.7101e-07 - acc: 1.000 - ETA: 0s - loss: 1.7059e-07 - acc: 1.000 - ETA: 0s - loss: 1.7033e-07 - acc: 1.000 - ETA: 0s - loss: 1.7003e-07 - acc: 1.000 - ETA: 0s - loss: 1.6966e-07 - acc: 1.000 - ETA: 0s - loss: 1.6956e-07 - acc: 1.000 - ETA: 0s - loss: 1.6917e-07 - acc: 1.000 - ETA: 0s - loss: 1.6866e-07 - acc: 1.000 - ETA: 0s - loss: 1.6824e-07 - acc: 1.000 - ETA: 0s - loss: 1.6780e-07 - acc: 1.000 - ETA: 0s - loss: 1.6775e-07 - acc: 1.000 - ETA: 0s - loss: 1.6741e-07 - acc: 1.000 - 5s 199us/sample - loss: 1.6719e-07 - acc: 1.0000 - val_loss: 1.2351 - val_acc: 0.8274\n",
      "Epoch 6/10\n",
      "23008/25000 [==========================>...] - ETA: 5s - loss: 1.2293e-07 - acc: 1.000 - ETA: 3s - loss: 1.0679e-07 - acc: 1.000 - ETA: 3s - loss: 1.0188e-07 - acc: 1.000 - ETA: 3s - loss: 9.8008e-08 - acc: 1.000 - ETA: 3s - loss: 9.9589e-08 - acc: 1.000 - ETA: 3s - loss: 1.0052e-07 - acc: 1.000 - ETA: 3s - loss: 9.9580e-08 - acc: 1.000 - ETA: 3s - loss: 1.0117e-07 - acc: 1.000 - ETA: 3s - loss: 1.0818e-07 - acc: 1.000 - ETA: 3s - loss: 1.0747e-07 - acc: 1.000 - ETA: 3s - loss: 1.0659e-07 - acc: 1.000 - ETA: 3s - loss: 1.0880e-07 - acc: 1.000 - ETA: 3s - loss: 1.0934e-07 - acc: 1.000 - ETA: 3s - loss: 1.0955e-07 - acc: 1.000 - ETA: 2s - loss: 1.0888e-07 - acc: 1.000 - ETA: 2s - loss: 1.1033e-07 - acc: 1.000 - ETA: 2s - loss: 1.1205e-07 - acc: 1.000 - ETA: 2s - loss: 1.1173e-07 - acc: 1.000 - ETA: 2s - loss: 1.1192e-07 - acc: 1.000 - ETA: 2s - loss: 1.1088e-07 - acc: 1.000 - ETA: 2s - loss: 1.1068e-07 - acc: 1.000 - ETA: 2s - loss: 1.1134e-07 - acc: 1.000 - ETA: 2s - loss: 1.1105e-07 - acc: 1.000 - ETA: 2s - loss: 1.1091e-07 - acc: 1.000 - ETA: 2s - loss: 1.1043e-07 - acc: 1.000 - ETA: 2s - loss: 1.1006e-07 - acc: 1.000 - ETA: 2s - loss: 1.1020e-07 - acc: 1.000 - ETA: 2s - loss: 1.0951e-07 - acc: 1.000 - ETA: 2s - loss: 1.0888e-07 - acc: 1.000 - ETA: 2s - loss: 1.0781e-07 - acc: 1.000 - ETA: 2s - loss: 1.0816e-07 - acc: 1.000 - ETA: 2s - loss: 1.0776e-07 - acc: 1.000 - ETA: 2s - loss: 1.0690e-07 - acc: 1.000 - ETA: 1s - loss: 1.0679e-07 - acc: 1.000 - ETA: 1s - loss: 1.0612e-07 - acc: 1.000 - ETA: 1s - loss: 1.0638e-07 - acc: 1.000 - ETA: 1s - loss: 1.0595e-07 - acc: 1.000 - ETA: 1s - loss: 1.0569e-07 - acc: 1.000 - ETA: 1s - loss: 1.0527e-07 - acc: 1.000 - ETA: 1s - loss: 1.0470e-07 - acc: 1.000 - ETA: 1s - loss: 1.0433e-07 - acc: 1.000 - ETA: 1s - loss: 1.0410e-07 - acc: 1.000 - ETA: 1s - loss: 1.0366e-07 - acc: 1.000 - ETA: 1s - loss: 1.0350e-07 - acc: 1.000 - ETA: 1s - loss: 1.0331e-07 - acc: 1.000 - ETA: 1s - loss: 1.0282e-07 - acc: 1.000 - ETA: 1s - loss: 1.0285e-07 - acc: 1.000 - ETA: 1s - loss: 1.0329e-07 - acc: 1.000 - ETA: 1s - loss: 1.0310e-07 - acc: 1.000 - ETA: 1s - loss: 1.0241e-07 - acc: 1.000 - ETA: 1s - loss: 1.0226e-07 - acc: 1.000 - ETA: 1s - loss: 1.0204e-07 - acc: 1.000 - ETA: 0s - loss: 1.0148e-07 - acc: 1.000 - ETA: 0s - loss: 1.0096e-07 - acc: 1.000 - ETA: 0s - loss: 1.0105e-07 - acc: 1.000 - ETA: 0s - loss: 1.0111e-07 - acc: 1.000 - ETA: 0s - loss: 1.0056e-07 - acc: 1.000 - ETA: 0s - loss: 1.0093e-07 - acc: 1.000 - ETA: 0s - loss: 1.0066e-07 - acc: 1.000 - ETA: 0s - loss: 1.0057e-07 - acc: 1.000 - ETA: 0s - loss: 1.0061e-07 - acc: 1.000 - ETA: 0s - loss: 1.0045e-07 - acc: 1.000 - ETA: 0s - loss: 1.0011e-07 - acc: 1.000 - ETA: 0s - loss: 9.9836e-08 - acc: 1.000 - ETA: 0s - loss: 9.9433e-08 - acc: 1.000 - ETA: 0s - loss: 9.9179e-08 - acc: 1.0000"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vecs.tsv')\n",
    "    files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11], [], [1429], [968], [4], [1538], [1538], [4738], [], [790], [2013], [11], [2921], [2188], [], [790], [2013], [11], [579], [], [11], [579], [], [4], [1783], [4], [4508], [11], [2921], [1278], [], [], [2013], [1005], [2921], [968], [579], [790], []]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I really think this is amazing. honest.\"\n",
    "sequence = tokenizer.texts_to_sequences(sentence)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with this project\n"
     ]
    }
   ],
   "source": [
    "print(\"done with this project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
