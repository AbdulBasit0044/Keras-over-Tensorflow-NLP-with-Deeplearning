{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Warning: Setting shuffle_files=True because split=TRAIN and shuffle_files=None. This behavior will be deprecated on 2019-08-06, at which point shuffle_files=False will be the default for all splits.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>\n",
      "<DatasetV1Adapter shapes: ((), ()), types: (tf.string, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(str(s.numpy()))\n",
    "    training_labels.append(l.numpy())\n",
    "    \n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(str(s.numpy()))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up hyperparameters\n",
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token = oov_tok)\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b i am a huge woody allen fan and so when i saw that this was playing at the cinema i couldn't help myself i wanted to see how allen would follow up his magnificent film match point seeing as this is another one of his films shot in g b which is unique among <OOV> work along with what seems to be his new <OOV> scarlett <OOV> scoop is much lighter than <OOV> and the humor is <OOV> most enjoyable aspect the plot revolves around <OOV> character a <OOV> student who gets a tip on a hot story from beyond the grave she falls in love with a suspected serial killer jackman and she must decide whether the truth\n",
      "b\"I am a huge Woody Allen fan and so when I saw that this was playing at the cinema I couldn't help myself. I wanted to see how Allen would follow up his magnificent film Match Point seeing as this is another one of his films shot in G.B. (which is unique among Allen's work) along with what seems to be his new muse Scarlett Johanson. Scoop is much lighter than MP and the humor is Scoop's most enjoyable aspect. The plot revolves around Johanson's character (a journalism student) who gets a tip on a hot story from beyond the grave. She falls in love with a suspected serial killer (Jackman) and she must decide whether the truth is worth finding. Oh and all of this is done with the help of a bumbling magician turned detective played by Allen.<br /><br />I must say that I thoroughly enjoyed Johanson's performance but I am a bit bias, I could watch a three hour film with Johanson in ever frame and remain enchanted. She plays a ditsy, yappy, bumbling sweetheart that is kind of a variation in a sense of Allen's stereotypical neurosis stricken character. She adds appropriate body language for comic effect. Needless to say almost anyone who sees this will find Johanson's character sickeningly cute and that is a plus.<br /><br />Allen is Allen... He is still playing the same character much like Chaplin and his Little Tramp character. Something that occur in this film makes me wonder if I will see the neurotic little hypochondriac again however. He is not in the cast of his next picture and has been spending more time exclusively behind the camera as of late...<br /><br />Jackman is also enjoyable as the suave, millionaire murder suspect. I cannot say that Jackman does anything in particular to make the role his but he suits his character none the less.<br /><br />In terms of the plot I cannot help but feel that this is fresh... In fact it stinks of Curse of the Jade Scorpion. Johanson and Allen are more detective-like than anything. However I must applaud Allen on his ending because it is a bit more clever than your typical unoutstanding Hollywood version of this film. Instead of everything being black and white, things are painted in shades of gray. Being entirely innocent has nothing to do with it nor does unequivocal guilt. Though the plot seemed old Woody still has a knack for one liners. I did find his allusions to his last film interesting... Come for the humor, laugh and be merry.<br /><br />Needless to say if you enjoy Allen's work watch it. If not watch something else...\"\n"
     ]
    }
   ],
   "source": [
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "def decode_review(text):\n",
    "    return ' '.join([reverse_word_index.get(i, '?') for i in text])\n",
    "\n",
    "print(decode_review(padded[1]))\n",
    "print(training_sentences[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid') \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 120, 16)           160000    \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1920)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 6)                 11526     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 7         \n",
      "=================================================================\n",
      "Total params: 171,533\n",
      "Trainable params: 171,533\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/10\n",
      "25000/25000 [==============================] - 5s 203us/sample - loss: 1.1179e-04 - acc: 1.0000 - val_loss: 0.8373 - val_acc: 0.8273\n",
      "Epoch 2/10\n",
      "25000/25000 [==============================] - 5s 187us/sample - loss: 6.9685e-05 - acc: 1.0000 - val_loss: 0.8734 - val_acc: 0.8277\n",
      "Epoch 3/10\n",
      "25000/25000 [==============================] - 5s 205us/sample - loss: 4.6716e-05 - acc: 1.0000 - val_loss: 0.9101 - val_acc: 0.8263\n",
      "Epoch 4/10\n",
      "25000/25000 [==============================] - 6s 227us/sample - loss: 2.6647e-05 - acc: 1.0000 - val_loss: 0.9409 - val_acc: 0.8274\n",
      "Epoch 5/10\n",
      "25000/25000 [==============================] - 6s 226us/sample - loss: 1.6773e-05 - acc: 1.0000 - val_loss: 0.9720 - val_acc: 0.8272\n",
      "Epoch 6/10\n",
      "25000/25000 [==============================] - 5s 217us/sample - loss: 1.0663e-05 - acc: 1.0000 - val_loss: 1.0027 - val_acc: 0.8268\n",
      "Epoch 7/10\n",
      "25000/25000 [==============================] - 5s 202us/sample - loss: 6.8060e-06 - acc: 1.0000 - val_loss: 1.0322 - val_acc: 0.8272\n",
      "Epoch 8/10\n",
      "25000/25000 [==============================] - 5s 207us/sample - loss: 4.3831e-06 - acc: 1.0000 - val_loss: 1.0615 - val_acc: 0.8268\n",
      "Epoch 9/10\n",
      "25000/25000 [==============================] - 5s 201us/sample - loss: 2.7711e-06 - acc: 1.0000 - val_loss: 1.0896 - val_acc: 0.8265\n",
      "Epoch 10/10\n",
      "25000/25000 [==============================] - 5s 207us/sample - loss: 1.7651e-06 - acc: 1.0000 - val_loss: 1.1173 - val_acc: 0.8267\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2b4a650a5f8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 16)\n"
     ]
    }
   ],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "for word_num in range(1, vocab_size):\n",
    "  word = reverse_word_index[word_num]\n",
    "  embeddings = weights[word_num]\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from google.colab import files\n",
    "except ImportError:\n",
    "    pass\n",
    "else:\n",
    "    files.download('vecs.tsv')\n",
    "    files.download('meta.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11], [], [1431], [968], [4], [1537], [1537], [4737], [], [790], [2015], [11], [2920], [2191], [], [790], [2015], [11], [579], [], [11], [579], [], [4], [1783], [4], [4506], [11], [2920], [1278], [], [], [2015], [1005], [2920], [968], [579], [790], []]\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I really think this is amazing. honest.\"\n",
    "sequence = tokenizer.texts_to_sequences(sentence)\n",
    "print(sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done with this project\n"
     ]
    }
   ],
   "source": [
    "print(\"done with this project\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
